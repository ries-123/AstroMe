{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f05276-d17d-4335-9da1-4503d1069218",
   "metadata": {},
   "source": [
    "# Class 9: LLM API Basics\n",
    "## Objective: Introduction to LLM API usage\n",
    "\n",
    "\n",
    "**Instructions:** Work with one or more students at your table. Discuss the key concepts and the code logic with one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558627d7-76fb-4a12-8f5d-069f4dfb7c03",
   "metadata": {},
   "source": [
    "## This code block loads the main modules and checks the setup\n",
    "\n",
    "Before you begin, you should have downloaded the `.env` and `.gitignore` files from carmen and put them in the same directory as your class notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3291c318-cf02-4309-b191-d005f1cb4bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded Astronomy 1221 key\n",
      "Successfully found .gitignore in the current directory\n",
      "Confirmed that .gitignore has the .env exclusion\n"
     ]
    }
   ],
   "source": [
    "# We will use this to suppress some warnings that are not important\n",
    "import warnings\n",
    "\n",
    "# Suppress specific Pydantic warnings that clutter the output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "\n",
    "# We will use dotenv to read the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# This import will return an error if LiteLLM is not installed \n",
    "import litellm\n",
    "import os\n",
    "\n",
    "# Use this to measure response time\n",
    "import time\n",
    "\n",
    "# URL of Ohio State's LiteLLM proxy server\n",
    "custom_api_base = \"https://litellmproxy.osu-ai.org\" \n",
    "\n",
    "# Our API key for Astronomy 1221 (keep this private to our class)\n",
    "astro1221_key = os.getenv(\"ASTRO1221_API_KEY\")\n",
    "if astro1221_key:\n",
    "    print(\"Successfully loaded Astronomy 1221 key\")\n",
    "else:\n",
    "    print(\"Error: did not find key. Check that .env exists in the same folder/directory as your class notebooks\")\n",
    "\n",
    "# Check that .gitignore exists in this directory\n",
    "if os.path.isfile('.gitignore'):\n",
    "    print(\"Successfully found .gitignore in the current directory\")\n",
    "else:\n",
    "    print(\"Error: Did not find .gitignore. Please download .gitignore from carmen and put in the same folder/directory as your class notebooks.\")\n",
    "\n",
    "with open('.gitignore', 'r') as f:\n",
    "    content = f.read()\n",
    "    if '.env' in content:\n",
    "        print(\"Confirmed that .gitignore has the .env exclusion\")\n",
    "    else: \n",
    "        print(\"Error: Did not find .env in .gitignore. Please download .gitignore from carmen and put with your class notebooks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb70d8b0-42ec-4918-856e-1d6eadd4b342",
   "metadata": {},
   "source": [
    "## Section 1: Test LiteLLM\n",
    "\n",
    "**Purpose:** Install and then test your LiteLLM setup\n",
    "\n",
    "If you have not already installed LiteLLM, do the following in a JupyterLab **terminal** on your computer:\n",
    "\n",
    "`pip install litellm`\n",
    "\n",
    "\n",
    "\n",
    "In addition, make sure you have downloaded both the `.env` and `.gitignore` files from carmen and put both files in the same directory as your class notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f9f425-2ceb-45e8-a0c3-7fed147ca693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to query a model with LiteLLM\n",
    "\n",
    "def prompt_llm(messages, model=\"openai/GPT-4.1-mini\", temperature=0.2, max_tokens=1000):\n",
    "    \"\"\"\n",
    "    Send a prompt or conversation to an LLM using LiteLLM and return the response.\n",
    "\n",
    "    Parameters:\n",
    "        messages: Either a string (single user prompt) or a list of message dicts with\n",
    "                  \"role\" and \"content\". If a string, formatted as [{\"role\": \"user\", \"content\": messages}].\n",
    "        model (str, optional): The name of the model to use. Defaults to \"openai/GPT-4.1-mini\".\n",
    "        temperature (float, optional): Value between 0 and 2; higher values make output more random. Defaults to 0.2.\n",
    "        max_tokens (int, optional): Maximum number of tokens to generate in the completion. Must be a positive integer. Defaults to 1000.\n",
    "\n",
    "    Prints the answer returned by the model.\n",
    "    \n",
    "    Returns:\n",
    "        response: The full response object from LiteLLM.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `temperature` is not in [0, 2] or `max_tokens` is not a positive integer.\n",
    "    \"\"\"\n",
    "    # If messages is a string, format it as a single user message\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    # Validate temperature\n",
    "    if not (isinstance(temperature, (int, float)) and 0 <= temperature <= 2):\n",
    "        raise ValueError(\"temperature must be a float between 0 and 2 (inclusive).\")\n",
    "    # Validate max_tokens\n",
    "    if not (isinstance(max_tokens, int) and max_tokens > 0):\n",
    "        raise ValueError(\"max_tokens must be a positive integer.\")\n",
    "\n",
    "    try: \n",
    "        print(\"Contacting LLM via University Server...\")\n",
    "\n",
    "        response = litellm.completion(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            api_base=custom_api_base,\n",
    "            api_key=astro1221_key,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "\n",
    "        answer = response['choices'][0]['message']['content']\n",
    "        print(f\"\\nSUCCESS! Here is the answer from {model}:\\n\")\n",
    "        print(answer)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: Could not connect. Details:\\n{e}\")    \n",
    "        response = None\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e87ae0-ee10-4258-b2ff-1b534c3cddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contacting LLM via University Server...\n",
      "\n",
      "SUCCESS! Here is the answer from openai/GPT-4.1-mini:\n",
      "\n",
      "A Type Ia supernova occurs from the thermonuclear explosion of a white dwarf in a binary system, while a core-collapse supernova results from the gravitational collapse of a massive starâ€™s iron core.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt to test the connection\n",
    "prompt = \"Explain the difference between a Type Ia and a core-collapse supernova in one sentence.\"\n",
    "\n",
    "response = prompt_llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c995d2-a2a3-4efc-9036-49611f1824a9",
   "metadata": {},
   "source": [
    "**Exercise:** Try a different prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf2607-da88-4bad-8f4c-214d8ab0bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to test the connection\n",
    "prompt = \"Provide an example prompt to test an LLM through an API interface suitable for an introductory astronomy data analysis course. Answer with just a few sentences.\"\n",
    "\n",
    "response = prompt_llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb863e-e4b1-4e61-b47a-19975288a671",
   "metadata": {},
   "source": [
    "## Section 2: Content of the response\n",
    "\n",
    "**Purpose:** The response includes lots of metadata, including the number of tokens you used, the model, and the cost of the query. \n",
    "\n",
    "The LiteLLM response is stored as a **PyDantic Model** called `ModelResponse`. We'll convert this to a dictionary and then look at the contents.\n",
    "\n",
    "Run the cell below and examine the query metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065f552-93f4-4fb8-bdc6-348a896af905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_response_metadata(response):\n",
    "    '''\n",
    "    Convert the response to a dictionary\n",
    "    Print information about token usage and costs\n",
    "    '''\n",
    "    \n",
    "    # Here are the top level keys\n",
    "    response_dict = response.model_dump()\n",
    "    print(f\"Top-level keys: {response_dict.keys()}\\n\")\n",
    "    \n",
    "    # Here are more details: \n",
    "    # 1. Get the exact model version used by the server\n",
    "    used_model = response.model\n",
    "    \n",
    "    # 2. Extract token counts from the 'usage' attribute\n",
    "    input_tokens = response.usage.prompt_tokens\n",
    "    output_tokens = response.usage.completion_tokens\n",
    "    total_tokens = response.usage.total_tokens\n",
    "    \n",
    "    # 3. Calculate the cost (LiteLLM does the math based on current rates)\n",
    "    cost = litellm.completion_cost(completion_response=response)\n",
    "    \n",
    "    print(f\"--- Query Metadata ---\")\n",
    "    print(f\"Model:        {used_model}\")\n",
    "    print(f\"Input Tokens: {input_tokens}\")\n",
    "    print(f\"Output Tokens:{output_tokens}\")\n",
    "    print(f\"Total Tokens: {total_tokens}\")\n",
    "    print(f\"Estimated Cost: ${cost:.6f}\") # Showing 6 decimal places for small queries\n",
    "\n",
    "show_response_metadata(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d2a5c-c877-4836-9e93-5b528d43da70",
   "metadata": {},
   "source": [
    "## Section 3: Model Comparison\n",
    "\n",
    "**Purpose:** Compare the answers and costs of using three models\n",
    "\n",
    "**Exercise:** Run the code block below and compare the following between the models:\n",
    "- Qualify of Response\n",
    "- Input Tokens\n",
    "- Output Tokens\n",
    "- Estimated Cost\n",
    "- Elapsed Time\n",
    "\n",
    "All are important considerations when choosing which model to use in your future code. \n",
    "\n",
    "**The bottom line is that you should choose the fastest and cheapest model that produces good enough results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef72761-d7c2-4b5e-b217-a2181b3dbb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will compare these models\n",
    "models = [\"gemini/gemini-2.5-flash\", \"openai/GPT-4.1-mini\", \"gemini/gemini-2.5-pro\"]\n",
    "\n",
    "# We will give each model the same prompt\n",
    "prompt = \"Explain the difference between a Type Ia and a core-collapse supernova in one sentence.\"\n",
    "\n",
    "for model in models: \n",
    "    print('-' * 60)\n",
    "    print(f\"\\nQuerying model {model}: \")\n",
    "    start_time = time.time()\n",
    "    response = prompt_llm(prompt, model=model)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    show_response_metadata(response)\n",
    "    print(f\"Completed model {model} in {elapsed_time:.2f}s\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b7285-b1ef-4b9c-b073-ae3117a2b0a7",
   "metadata": {},
   "source": [
    "## Section 4: Tuning the Model\n",
    "\n",
    "The `prompt_llm()` function above has two other parameters: `temperature` and `max_tokens`. \n",
    "\n",
    "The `temperature` parameter is the creativity dial. The range is generally from 0.0 to 2.0, which corresponds to very deterministic to very creative. For scientific research, you should use a small value of temperature (0.0 to 0.3).\n",
    "\n",
    "The `max_tokens` is a length limit that keeps the model from rambling and helps control costs. \n",
    "\n",
    "Run the cell below and compare the output from the same prompt with three different temperatures. Especially look at the response with temperature = 1.8 and in ModelResponse look for `finish_reason`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d809f9-7a8a-4607-bbe1-9ff69aaf4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to test the temperature parameter\n",
    "prompt = \"Describe the life of a high-mass star, from its birth in a nebula to its death in a supernova. Write this from the perspective of the star itself.\"\n",
    "\n",
    "temperature_values = [0.0, 0.3, 1.8]\n",
    "for temperature in temperature_values:\n",
    "    print(f\"Temperature = {temperature}\")  \n",
    "    response = prompt_llm(prompt, temperature=temperature)\n",
    "    show_response_metadata(response)\n",
    "    print('=' * 60)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e1735",
   "metadata": {},
   "source": [
    "**Test your understanding:** Experiment with the output with different temperature values and different prompts with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37703842",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is right ascension?\" # Experiment\n",
    "temperature = 0.2 # Adjust to experiment\n",
    "response = prompt_llm(prompt, temperature=temperature)\n",
    "show_response_metadata(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f893a",
   "metadata": {},
   "source": [
    "## Section 5: State and Memory\n",
    "\n",
    "LLMs are technically stateless, which means they do not remember anything between prompts. To create the illusion of memory, they need to be told the entire conversation history with each new prompt. This can turn into a lot of token usage fairly quickly. \n",
    "\n",
    "Here is an example of how to manage this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize your \"Memory\" list with a System Message\n",
    "chat_memory = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful astronomy research assistant appropriate for an introductory astronomy data analysis course.\"}\n",
    "]\n",
    "\n",
    "def chat_with_memory(user_input):\n",
    "    # 2. Add the User's new message to memory\n",
    "    chat_memory.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # 3. Send the WHOLE list to the LLM (uses notebook's custom_api_base and astro1221_key)\n",
    "    response = prompt_llm(chat_memory)\n",
    "\n",
    "    # 4. Extract the answer (handle None if connection failed)\n",
    "    if response is None:\n",
    "        return None\n",
    "    ai_answer = response.choices[0].message.content\n",
    "\n",
    "    # 5. CRITICAL: Add the AI's own answer to memory so it remembers what it said\n",
    "    chat_memory.append({\"role\": \"assistant\", \"content\": ai_answer})\n",
    "\n",
    "    return ai_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd23cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "prompt1 = \"What is the distance to Andromeda?\"\n",
    "print(f\"\\nAsking about question 1: {prompt1}\")\n",
    "print(chat_with_memory(prompt1))\n",
    "print('=' * 60)\n",
    "\n",
    "prompt2 = \"How much further is that than the Moon?\"\n",
    "print(f\"\\nAsking about question 2: {prompt2}\")\n",
    "print(chat_with_memory(prompt2)) # AI remembers Andromeda context\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c3ed4",
   "metadata": {},
   "source": [
    "**Test your understanding:** Ask prompt2 again with `prompt_llm(prompt2)` and compare to the previous response. What is different? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ecd342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask question 2 without memory\n",
    "prompt2 = \"How much further is that than the Moon?\"\n",
    "response = prompt_llm(prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64075c32",
   "metadata": {},
   "source": [
    "## Section 6: A chat agent\n",
    "\n",
    "Adding all of the previous chats to the next one can quickly lead to ballooning token counts. As chats get longer, one strategy is to use the LLM to summarize the previous chats and use these summaries rather than all of the previous tokens. \n",
    "\n",
    "Here is an `AstronomyChatAgent` `class` that demonstrates how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AstronomyChatAgent:\n",
    "    \"\"\"Chat agent that uses this notebook's prompt_llm, custom_api_base, and astro1221_key.\"\"\"\n",
    "\n",
    "    def __init__(self, model=\"gemini/gemini-2.5-flash\", system_prompt=None):\n",
    "        self.model = model\n",
    "        # Uses notebook globals: custom_api_base, astro1221_key (set in first code cell)\n",
    "        if system_prompt is None:\n",
    "            system_prompt = \"You are a helpful Astronomy TA. Be concise and scientific.\"\n",
    "        self.history = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        self.summary = \"\" # This will store the summary of the chat history\n",
    "\n",
    "    def chat(self, user_input):\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        if len(self.history) > 10: # This is the number of chats before summarizing\n",
    "            self._summarize_history() \n",
    "\n",
    "        # Use notebook's prompt_llm (same API base and key)\n",
    "        response = prompt_llm(self.history, model=self.model)\n",
    "        if response is None:\n",
    "            self.history.pop()  # remove the user message we just added\n",
    "            return None\n",
    "        ai_answer = response.choices[0].message.content\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": ai_answer})\n",
    "        return ai_answer\n",
    "\n",
    "    def _summarize_history(self):\n",
    "        \"\"\"Compress old messages into a summary using prompt_llm.\"\"\"\n",
    "        print(\"\\n--- Memory full. Summarizing previous context... ---\")\n",
    "        messages_to_summarize = self.history[1:-2]\n",
    "        summary_prompt = f\"Summarize the key astronomical facts discussed so far: {messages_to_summarize}\"\n",
    "        sum_res = prompt_llm(summary_prompt, model=\"openai/GPT-4.1-mini\")\n",
    "        if sum_res is None:\n",
    "            self.summary = \"(Summary failed.)\"\n",
    "        else:\n",
    "            self.summary = sum_res.choices[0].message.content\n",
    "        self.history = [\n",
    "            self.history[0],\n",
    "            {\"role\": \"system\", \"content\": f\"Summary of previous chat: {self.summary}\"},\n",
    "            self.history[-2],\n",
    "            self.history[-1],\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab93b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (run the setup and prompt_llm cells first)\n",
    "agent = AstronomyChatAgent()\n",
    "print(\"AI:\", agent.chat(\"What is the main sequence on the HR diagram?\"))\n",
    "print(\"AI:\", agent.chat(\"Which star stays on it the longest?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46e142",
   "metadata": {},
   "source": [
    "## Section 7: Image Analysis\n",
    "\n",
    "Astronomy often deals with image rather than text data, yet image data are typically encoded in binary format rather than strings. \n",
    "\n",
    "The solution is to convert the image into a **Base64** encoded string. This is a text representation of binary data. And then send this string in the query. \n",
    "\n",
    "First, we will download and display an image of the nearby galaxy M81 from this website: https://www.nasa.gov/image-article/spiral-galaxy-m81/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b78d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the public domain image\n",
    "img_url = \"https://www.nasa.gov/wp-content/uploads/2023/03/m81.jpg\"\n",
    "img_data = requests.get(img_url).content\n",
    "\n",
    "with open(\"m81_tiny.jpg\", \"wb\") as f:\n",
    "    f.write(img_data)\n",
    "\n",
    "print(\"Downloaded m81_tiny.png (Public Domain)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image with matplotlib\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(plt.imread(\"m81_tiny.jpg\"))\n",
    "ax.axis(\"off\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d758bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def analyze_astronomical_image(image_path, question):\n",
    "    \"\"\"\n",
    "    Encodes an image to Base64 and sends it to an LLM (Claude/Gemini) for analysis.\n",
    "    \"\"\"\n",
    "    # 1. Read the image as binary and encode to Base64\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        # Convert binary data to base64 string\n",
    "        image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# 2. Format the message for a Vision-capable model (OpenAI/Gemini Standard)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_data}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    # print(messages)\n",
    "\n",
    "    # 3. Call a Vision-capable model (like Gemini Flash)\n",
    "    response = litellm.completion(\n",
    "        model=\"gemini/gemini-2.5-flash\",\n",
    "        messages=messages,\n",
    "        api_base=custom_api_base,\n",
    "        api_key=astro1221_key\n",
    "    )\n",
    "\n",
    "    show_response_metadata(response)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20067a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "result = analyze_astronomical_image(\"m81_tiny.jpg\", \"What morphological features do you see in this galaxy?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00790d3",
   "metadata": {},
   "source": [
    "**Test your understanding:** Try a different question about this galaxy or try a different astronomical object. \n",
    "\n",
    "If you look for another object, I recommend searching for relatively small images to get faster responses and use fewer resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
